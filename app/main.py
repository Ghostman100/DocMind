from fastapi import FastAPI, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging
import uuid
from datetime import datetime, UTC
import langchain
import time

from .config import settings
from .schemas import (
    IngestRequest,
    IngestResponse,
    QueryRequest,
    QueryResponse,
    HealthResponse,
    SearchResult,
    EmbedRequest,
    EmbedResponse,
)
from .qdrant_client import QdrantService
from .embeddings import get_embedding_registry
from .chunking import ParagraphChunker, RecursiveChunker
from .scripts.doc_to_text import extract_text_from_file

# LangChain –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
from .langchain_integration.schemas import (
    LangChainIngestRequest,
    LangChainIngestResponse,
    LangChainQueryRequest,
    LangChainQueryResponse,
    LangChainSearchResult,
    SummarizeRequest,
    SummarizeResponse,
    SummarizeWithAnalysisResponse,
    ExtractPointsRequest,
    ExtractPointsResponse,
    ExtractedPoint,
)
from .langchain_integration.vector_store import get_langchain_vector_store
from .langchain_integration.summarizer import summarize_document, extract_points_from_document, summarize_text, apply_prompt_to_text

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
qdrant_service = None
embedding_registry = None
chunker = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """–°–æ–±—ã—Ç–∏—è –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    global qdrant_service, embedding_registry, chunker

    # –ó–∞–ø—É—Å–∫
    logger.info("Starting DocMind application...")

    # –í–∫–ª—é—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π debug —Ä–µ–∂–∏–º LangChain –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
    if settings.langchain_debug:
        langchain.debug = True
        logger.info("LangChain DEBUG mode enabled - detailed output will be shown")
    else:
        langchain.debug = False

    logger.info(f"Chunking strategy: {settings.chunking_strategy}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embedding registry –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
    logger.info(f"Loading {len(settings.embedding_models)} embedding models...")
    embedding_registry = get_embedding_registry()

    for model_name in settings.embedding_models:
        logger.info(f"Registering model: {model_name}")
        embedding_registry.register_model(model_name)

    logger.info(f"All models loaded. Default model: {settings.default_embedding_model}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant service —Å registry
    qdrant_service = QdrantService(embedding_registry)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if settings.chunking_strategy == "paragraph":
        chunker = ParagraphChunker()
        logger.info("Using ParagraphChunker")
    elif settings.chunking_strategy == "recursive":
        chunker = RecursiveChunker(
            chunk_size=settings.chunk_size,
            chunk_overlap=settings.chunk_overlap
        )
        logger.info(f"Using RecursiveChunker (size={settings.chunk_size}, overlap={settings.chunk_overlap})")
    else:
        raise ValueError(f"Unknown chunking strategy: {settings.chunking_strategy}")

    logger.info("Application started successfully")

    yield

    # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
    logger.info("Shutting down DocMind application...")


# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="DocMind",
    description="RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞",
    version="1.0.0",
    lifespan=lifespan
)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/", tags=["Health"])
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint"""
    return {
        "message": "DocMind API is running",
        "docs_url": "/docs",
        "health_check": "/health"
    }


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """Endpoint –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant
        qdrant_connected = True
        try:
            qdrant_service.client.get_collections()
        except Exception as e:
            logger.error(f"Qdrant connection failed: {e}")
            qdrant_connected = False

        return HealthResponse(
            status="healthy" if qdrant_connected else "degraded",
            qdrant_connected=qdrant_connected,
            embedding_models=embedding_registry.list_models(),
            default_embedding_model=settings.default_embedding_model,
            chunking_strategy=settings.chunking_strategy,
            langchain_debug=settings.langchain_debug
        )
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=str(e)
        )


@app.post("/ingest", response_model=IngestResponse, tags=["Documents"])
async def ingest_document(request: IngestRequest):
    """
    –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Å–∏—Å—Ç–µ–º—É

    –≠—Ç–æ—Ç endpoint:
    1. –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
    2. –†–∞–∑–±–∏–≤–∞–µ—Ç –µ–≥–æ –Ω–∞ —á–∞–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    3. –°–æ–∑–¥–∞–µ—Ç embeddings –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
    4. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫–∏ –≤ Qdrant
    """
    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å UUID –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document_id = str(uuid.uuid4())
        upload_timestamp = datetime.now(UTC)

        logger.info(f"Ingesting document: {request.document_name}")
        logger.info(f"Document ID: {document_id}")
        logger.info(f"Document length: {len(request.text)} characters")

        # –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
        chunks = chunker.chunk(request.text)
        logger.info(f"Created {len(chunks)} chunks")

        if not chunks:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No chunks were created from the document. The text might be too short."
            )

        # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —á–∞–Ω–∫–æ–≤
        chunk_texts = [chunk.text for chunk in chunks]
        chunk_metadata = [chunk.metadata for chunk in chunks]

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ Qdrant –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        chunks_count = qdrant_service.ingest_chunks(
            document_id=document_id,
            document_name=request.document_name,
            upload_timestamp=upload_timestamp,
            chunks=chunk_texts,
            model_name=settings.default_embedding_model,
            metadata=chunk_metadata
        )

        logger.info(f"Successfully ingested {chunks_count} chunks for document {document_id}")

        return IngestResponse(
            success=True,
            message=f"Successfully ingested {chunks_count} chunks",
            chunks_count=chunks_count,
            document_id=document_id
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error ingesting document: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to ingest document: {str(e)}"
        )


@app.post("/query", response_model=QueryResponse, tags=["Search"])
async def query_documents(request: QueryRequest):
    """
    –ó–∞–ø—Ä–æ—Å–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫

    –≠—Ç–æ—Ç endpoint:
    1. –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
    2. –°–æ–∑–¥–∞–µ—Ç embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    3. –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ —á–∞–Ω–∫–∏ –≤ Qdrant
    4. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
    """
    try:
        logger.info(f"Querying document: {request.document_id}")
        logger.info(f"Query: {request.query}")

        # –ü–æ–∏—Å–∫ –≤ Qdrant —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ document_id, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        results = qdrant_service.search(
            document_id=request.document_id,
            query=request.query,
            model_name=settings.default_embedding_model,
            top_k=request.top_k
        )

        logger.info(f"Found {len(results)} results")

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        search_results = [
            SearchResult(
                document=result["document"],
                score=result["score"],
                metadata=result["metadata"]
            )
            for result in results
        ]

        return QueryResponse(
            success=True,
            results=search_results,
            query=request.query,
            document_id=request.document_id
        )

    except Exception as e:
        logger.error(f"Error querying documents: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to query documents: {str(e)}"
        )


@app.post("/embed", response_model=EmbedResponse, tags=["Embeddings"])
async def get_embedding(request: EmbedRequest):
    """
    –ü–æ–ª—É—á–∏—Ç—å embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞

    –≠—Ç–æ—Ç endpoint:
    1. –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    2. –°–æ–∑–¥–∞–µ—Ç embedding –∏—Å–ø–æ–ª—å–∑—É—è —É–∫–∞–∑–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    3. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä embedding —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    try:
        logger.info(f"Creating embedding with model: {request.model}")
        logger.info(f"Text length: {len(request.text)} characters")

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞
        if not embedding_registry.has_model(request.model):
            available_models = embedding_registry.list_models()
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Model '{request.model}' not available. Available models: {available_models}"
            )

        # –ü–æ–ª—É—á–∏—Ç—å embedding service –¥–ª—è –º–æ–¥–µ–ª–∏
        embedding_service = embedding_registry.get_model(request.model)

        # –°–æ–∑–¥–∞—Ç—å embedding
        embedding_vector = embedding_service.encode_single(request.text)

        logger.info(f"Successfully created embedding with dimension {len(embedding_vector)}")

        return EmbedResponse(
            embedding=embedding_vector.tolist(),
            model=request.model,
            dimension=len(embedding_vector)
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating embedding: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create embedding: {str(e)}"
        )


@app.post("/test", response_model=IngestResponse, tags=["Documents"])
async def test_ingest_document():
    """
    –¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ test.docx

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ test.docx –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –µ–≥–æ –≤ —Å–∏—Å—Ç–µ–º—É
    """
    try:
        # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ test.docx
        text = extract_text_from_file('test.docx')

        if not text or not text.strip():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Failed to extract text from test.docx or file is empty"
            )

        # –°–æ–∑–¥–∞—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è ingest
        ingest_request = IngestRequest(
            document_name="test.docx",
            text=text
        )

        # –í—ã–∑–≤–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        return await ingest_document(ingest_request)

    except FileNotFoundError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="test.docx file not found"
        )

    except Exception as e:
        logger.error(f"Error in test_ingest_document: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to process test.docx: {str(e)}"
        )


@app.post("/test_summarize", response_model=SummarizeWithAnalysisResponse, tags=["LangChain"])
async def test_summarize_document(target_tokens: int = 70000):
    """
    –¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ test.docx —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –∞–Ω–∞–ª–∏–∑–æ–º

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ test.docx, —Å–æ–∑–¥–∞–µ—Ç —Ä–µ–∑—é–º–µ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç PROMPT –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    """
    start_total = time.time()

    try:
        # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ test.docx
        start_extraction = time.time()
        text = extract_text_from_file('test.docx')
        extraction_time = time.time() - start_extraction

        if not text or not text.strip():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Failed to extract text from test.docx or file is empty"
            )

        logger.info(f"Extracted {len(text)} characters from test.docx in {extraction_time:.2f}s")

        # –°–æ–∑–¥–∞—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è summarize
        summarize_request = SummarizeRequest(
            text=text,
            target_tokens=target_tokens
        )

        # –í—ã–∑–≤–∞—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é
        start_summarization = time.time()
        summary_result = await langchain_summarize_text(summarize_request)
        summarization_time = time.time() - start_summarization

        logger.info(f"Summary completed in {summarization_time:.2f}s, now applying PROMPT for analysis")

        # –ü—Ä–∏–º–µ–Ω–∏—Ç—å PROMPT –∫ —Å—É–º–º–∞—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
        start_analysis = time.time()
        analysis_result = apply_prompt_to_text(
            text=summary_result.summary,
            prompt=PROMPT
        )
        analysis_time = time.time() - start_analysis

        total_time = time.time() - start_total

        logger.info(f"Analysis completed in {analysis_time:.2f}s")
        logger.info(f"Total time: {total_time:.2f}s (extraction: {extraction_time:.2f}s, summarization: {summarization_time:.2f}s, analysis: {analysis_time:.2f}s)")

        # –í–µ—Ä–Ω—É—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π –∏ –∞–Ω–∞–ª–∏–∑–æ–º
        return SummarizeWithAnalysisResponse(
            success=True,
            summary=summary_result.summary,
            analysis=analysis_result["result"],
            summary_tokens=summary_result.output_tokens,
            analysis_input_tokens=analysis_result["input_tokens"],
            analysis_output_tokens=analysis_result["output_tokens"],
            parts_processed=summary_result.parts_processed,
            strategy_used=summary_result.strategy_used,
            extraction_time_seconds=round(extraction_time, 2),
            summarization_time_seconds=round(summarization_time, 2),
            analysis_time_seconds=round(analysis_time, 2),
            total_time_seconds=round(total_time, 2)
        )

    except FileNotFoundError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="test.docx file not found"
        )

    except Exception as e:
        logger.error(f"Error in test_summarize_document: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to summarize test.docx: {str(e)}"
        )


@app.post("/test_langchain", response_model=LangChainIngestResponse, tags=["LangChain"])
async def test_ingest_langchain():
    """
    –¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ test.docx —á–µ—Ä–µ–∑ LangChain

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ test.docx –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –µ–≥–æ –≤ —Å–∏—Å—Ç–µ–º—É —á–µ—Ä–µ–∑ LangChain
    """
    try:
        # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ test.docx
        text = extract_text_from_file('test.docx')

        if not text or not text.strip():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Failed to extract text from test.docx or file is empty"
            )

        # –°–æ–∑–¥–∞—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è LangChain ingest
        ingest_request = LangChainIngestRequest(
            document_name="test.docx",
            text=text
        )

        # –í—ã–∑–≤–∞—Ç—å LangChain endpoint –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        return await langchain_ingest_document(ingest_request)

    except FileNotFoundError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="test.docx file not found"
        )

    except Exception as e:
        logger.error(f"Error in test_ingest_langchain: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to process test.docx via LangChain: {str(e)}"
        )


# ============================================================================
# LangChain Endpoints
# ============================================================================

@app.post("/langchain/ingest", response_model=LangChainIngestResponse, tags=["LangChain"])
async def langchain_ingest_document(request: LangChainIngestRequest):
    """
    –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Å–∏—Å—Ç–µ–º—É —á–µ—Ä–µ–∑ LangChain

    –≠—Ç–æ—Ç endpoint –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LangChain RecursiveCharacterTextSplitter
    –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ QdrantVectorStore –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
    """
    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å UUID –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document_id = str(uuid.uuid4())

        logger.info(f"[LangChain] Ingesting document: {request.document_name}")
        logger.info(f"[LangChain] Document ID: {document_id}")
        logger.info(f"[LangChain] Document length: {len(request.text)} characters")

        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LangChain text splitter
        from langchain_text_splitters import RecursiveCharacterTextSplitter

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.chunk_size,
            chunk_overlap=settings.chunk_overlap,
            separators=["\n\n", "\n", ". ", "! ", "? ", "; ", ", ", " ", ""]
        )

        texts = text_splitter.split_text(request.text)
        logger.info(f"[LangChain] Created {len(texts)} chunks")

        if not texts:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No chunks were created from the document. The text might be too short."
            )

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ Qdrant —á–µ—Ä–µ–∑ LangChain
        langchain_service = get_langchain_vector_store()
        chunks_count = langchain_service.ingest_texts(
            texts=texts,
            document_id=document_id,
            document_name=request.document_name
        )

        logger.info(f"[LangChain] Successfully ingested {chunks_count} chunks for document {document_id}")

        return LangChainIngestResponse(
            success=True,
            message=f"Successfully ingested {chunks_count} chunks via LangChain",
            chunks_count=chunks_count,
            document_id=document_id
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[LangChain] Error ingesting document: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to ingest document via LangChain: {str(e)}"
        )


@app.post("/langchain/query", response_model=LangChainQueryResponse, tags=["LangChain"])
async def langchain_query_documents(request: LangChainQueryRequest):
    """
    –ó–∞–ø—Ä–æ—Å–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—è LangChain retriever

    –≠—Ç–æ—Ç endpoint –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LangChain QdrantVectorStore.as_retriever()
    –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ document_id.
    """
    try:
        logger.info(f"[LangChain] Querying document: {request.document_id}")
        logger.info(f"[LangChain] Query: {request.query}")

        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ LangChain
        langchain_service = get_langchain_vector_store()
        results = langchain_service.search(
            document_id=request.document_id,
            query=request.query,
            top_k=request.top_k
        )

        logger.info(f"[LangChain] Found {len(results)} results")

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        search_results = [
            LangChainSearchResult(
                page_content=result["page_content"],
                score=result.get("score", 0.0),
                metadata=result["metadata"]
            )
            for result in results
        ]

        return LangChainQueryResponse(
            success=True,
            results=search_results,
            query=request.query,
            document_id=request.document_id
        )

    except Exception as e:
        logger.error(f"[LangChain] Error querying documents: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to query documents via LangChain: {str(e)}"
        )


@app.post("/langchain/summarize", response_model=SummarizeResponse, tags=["LangChain"])
async def langchain_summarize_text(request: SummarizeRequest):
    """
    –°–æ–∑–¥–∞—Ç—å —Ä–µ–∑—é–º–µ —Ç–µ–∫—Å—Ç–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏

    –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç ‚â§ 90 000 —Ç–æ–∫–µ–Ω–æ–≤ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç "stuff" –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å.
    –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç > 90 000 —Ç–æ–∫–µ–Ω–æ–≤ - –¥–µ–ª–∏—Ç –Ω–∞ 2-3 —á–∞—Å—Ç–∏ –∏ –¥–µ–ª–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ "stuff" —Å–≤–æ–¥–æ–∫.

    –¢—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LLM_API_KEY –≤ .env —Ñ–∞–π–ª–µ.
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ LangChain –≤–∫–ª—é—á–µ–Ω –∏ API –∫–ª—é—á –Ω–∞—Å—Ç—Ä–æ–µ–Ω
        if not settings.langchain_enabled:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="LangChain functionality is disabled. Set LANGCHAIN_ENABLED=true in .env"
            )

        if not settings.llm_api_key:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="LLM API key is not configured. Set LLM_API_KEY in .env"
            )

        logger.info(f"[LangChain] Summarizing text, target tokens: {request.target_tokens}")

        # –í—ã–ø–æ–ª–Ω–∏—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é
        result = summarize_text(
            text=request.text,
            target_tokens=request.target_tokens
        )

        logger.info(f"[LangChain] Summarization completed: {result['input_tokens']} -> {result['output_tokens']} tokens")

        return SummarizeResponse(
            success=True,
            summary=result["summary"],
            input_tokens=result["input_tokens"],
            output_tokens=result["output_tokens"],
            parts_processed=result["parts_processed"],
            strategy_used=result["strategy_used"]
        )

    except HTTPException:
        raise
    except ValueError as e:
        logger.error(f"[LangChain] Validation error: {e}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"[LangChain] Error summarizing text: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to summarize text: {str(e)}"
        )


@app.post("/langchain/extract_points", response_model=ExtractPointsResponse, tags=["LangChain"])
async def langchain_extract_points(request: ExtractPointsRequest):
    """
    –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ –ø—É–Ω–∫—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º —Ç–µ–º–∞–º

    –≠—Ç–æ—Ç endpoint –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–π —Ç–µ–º–µ
    –≤ —Ä–∞–º–∫–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Å—É–º–º–∞—Ä–∏–∑—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã.

    –¢—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LLM_API_KEY –≤ .env —Ñ–∞–π–ª–µ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ summarize=True).
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        if request.summarize:
            if not settings.langchain_enabled:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="LangChain functionality is disabled. Set LANGCHAIN_ENABLED=true in .env"
                )

            if not settings.llm_api_key:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="LLM API key is not configured. Set LLM_API_KEY in .env"
                )

        logger.info(f"[LangChain] Extracting points from document: {request.document_id}")
        logger.info(f"[LangChain] Topics: {len(request.topics)}")
        logger.info(f"[LangChain] Summarize: {request.summarize}")

        # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        result = extract_points_from_document(
            document_id=request.document_id,
            topics=request.topics,
            chunks_per_topic=request.chunks_per_topic,
            summarize=request.summarize
        )

        logger.info(f"[LangChain] Extraction completed")

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ Pydantic –º–æ–¥–µ–ª–∏
        extracted_points = [
            ExtractedPoint(
                topic=point["topic"],
                relevant_chunks=point["relevant_chunks"],
                summary=point.get("summary")
            )
            for point in result["extracted_points"]
        ]

        return ExtractPointsResponse(
            success=True,
            document_id=result["document_id"],
            extracted_points=extracted_points,
            total_chunks_retrieved=result["total_chunks_retrieved"]
        )

    except HTTPException:
        raise
    except ValueError as e:
        logger.error(f"[LangChain] Validation error: {e}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"[LangChain] Error extracting points: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to extract points: {str(e)}"
        )



if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=True
    )


PROMPT = """
–ú—ã –æ–∫–∞–∑—ã–≤–∞–µ–º —É—Å–ª—É–≥–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –Ω–∞ –ø—Ä–æ–µ–∫—Ç (–Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É). –¢—ã –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–Ω–¥–µ—Ä–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∏ –≤—ã–¥–∞—Ç—å –º–Ω–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å —Ü–∏—Ç–∞—Ç–∞–º–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –Ω–∞—Ä—É—à–∞–µ—Ç—Å—è –ø—É–Ω–∫—Ç –∏–∑ —á–µ–∫-–ª–∏—Å—Ç–∞ –¥–∞–ª–µ–µ) –Ω–∞ —Å–ª. –º–æ–º–µ–Ω—Ç—ã –≤ –¢–∞–±–ª–∏—Ü—É ‚Äù–ß–µ–∫-–ª–∏—Å—Ç –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã‚Äù:
–ü—É–Ω–∫—Ç 1. –ú—ã –Ω–µ –æ–∫–∞–∑—ã–≤–∞–µ–º —É—Å–ª—É–≥–∏ –ø—Ä–µ–¥–ø—Ä–æ–µ–∫—Ç–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ, –ø–æ–¥–¥–µ—Ä–∂–∫–∞, –ª–∏—Ü–µ–Ω–∑–∏–∏, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∞, –ø—Ä–æ–¥–∞–∂—É –ø—Ä–æ–≥—Ä–∞–º–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ —ç—Ç–∏ –∑–∞–¥–∞—á–∏ –∏ –≤–∏–¥—ã —É—Å–ª—É–≥ –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–∫–∞–∑–∞–Ω—ã –≤ —Ç–µ–Ω–¥–µ—Ä–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–∞–∫ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å –≤ —Ä–∞–º–∫–∞—Ö —Ç–µ–Ω–¥–µ—Ä–∞
–ü—É–Ω–∫—Ç 2.–ú—ã –Ω–µ –∏–º–µ–µ–º —Å—Ç–∞—Ç—É—Å –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ —ç—Ç–∏ —Å—Ç–∞—Ç—É—Å—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∫ —É—á–∞—Å—Ç–∏—é –≤ —Ç–µ–Ω–¥–µ—Ä–µ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (–Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —è–≤–Ω–æ –ø—Ä–æ–ø–∏—Å–∞–Ω–æ —Ç–∞–∫–∏—Ö —Å—Ç–∞—Ç—É—Å–æ–≤):
–ü–∞—Ä—Ç–Ω–µ—Ä ¬´–¶–µ–Ω—Ç—Ä –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ 1–°: –ö–û–†–ü
–ö–∞–Ω–¥–∏–¥–∞—Ç –≤ ¬´–¶–µ–Ω—Ç—Ä—ã –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ 1–°: –ö–û–†–ü¬ª
–ù–∞–ª–∏—á–∏–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ –Ω–∞ —Å–∞–π—Ç–µ –§–∏—Ä–º—ã 1–° https://1c.ru/ –≤ —Ä–∞–∑–¥–µ–ª–µ –†–µ–π—Ç–∏–Ω–≥ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤ ¬´1–°: –¶–µ–Ω—Ç—Ä—ã –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –ø–æ ERP-—Ä–µ—à–µ–Ω–∏—è–º¬ª:
–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å 1–°: –§—Ä–∞–Ω—á–∞–π–∑–∏–Ω–≥.
–°—Ç–∞—Ç—É—Å –¶–µ–Ω—Ç—Ä —Ä–µ–∞–ª—å–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
–¶–µ–Ω—Ç—Ä –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –ø–æ ERP-—Ä–µ—à–µ–Ω–∏—è–º –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ–º
¬≠–ü–∞—Ä—Ç–Ω–µ—Ä –ø–æ –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–º—É –∫–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥—É
¬≠–ü–∞—Ä—Ç–Ω–µ—Ä –ø–æ —É–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–æ–º—É –∫–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥—É
¬≠–¶–µ–Ω—Ç—Ä —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Ñ–∏—Ä–º—ã 1C
¬≠–¶–µ–Ω—Ç—Ä —Å–µ—Ä—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ñ–∏—Ä–º—ã 1C
—Å—Ç–∞—Ç—É—Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞ 1–° ¬´–¶–µ–Ω—Ç—Ä ERP-—Ä–µ—à–µ–Ω–∏–π
–ö–∞–Ω–¥–∏–¥–∞—Ç—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å 1–° - ¬´–ö–∞–Ω–¥–∏–¥–∞—Ç –≤ 1–°: –ö–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥¬ª
¬´–ö–∞–Ω–¥–∏–¥–∞—Ç –≤ 1–°: –¶–µ–Ω—Ç—Ä ERP¬ª
¬´–ü–∞—Ä—Ç–Ω–µ—Ä –ø–æ –≤–Ω–µ–¥—Ä–µ–Ω–∏—é –∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º—É –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—é –†–µ—à–µ–Ω–∏–π –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ 1–°: –ü—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ 8¬ª.
–ü—É–Ω–∫—Ç 3. –ú—ã –Ω–µ –æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É SLA 1 –∏ 2 –ª–∏–Ω–∏–∏, 24/7, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ –µ–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —É–∫–∞–∑–∞–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ç–æ–≥–æ, —á—Ç–æ —è–≤–Ω –∫ —ç—Ç–æ–º—É –≤–µ–¥–µ—Ç( –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª-—Ü–µ–Ω—Ç—Ä–∞ —É –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è, –≤—ã–¥–µ–ª–µ–Ω–Ω–∞—è —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Ç.–¥)
–ü—É–Ω–∫—Ç 4. –í —Ç–µ–Ω–¥–µ—Ä–µ –Ω–µ –¥–æ–ª–∂–Ω–æ —Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –û—á–Ω–æ–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –ó–∞–∫–∞–∑—á–∏–∫–∞ –¥–ª—è –æ–∫–∞–∑–∞–Ω–∏—è —É—Å–ª—É–≥, –≤—ã–µ–∑–¥—ã –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—é –∏ –¥—Ä. —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ —è–≤–Ω–æ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞—é—â–∏–µ –≤—ã–µ–∑–¥ –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –ó–∞–∫–∞–∑—á–∏–∫–∞ –¥–ª—è –æ–∫–∞–∑–∞–Ω–∏—è —É—Å–ª—É–≥, –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤—ã–µ–∑–¥–æ–≤ –ø–æ-–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ —Ç.–¥. –í—Å—è —Ä–∞–±–æ—Ç–∞ –ø–æ –æ–∫–∞–∑–∞–Ω–∏—é —É—Å–ª—É–≥ –¥–æ–ª–∂–Ω–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—å—Å—è —É–¥–∞–ª–µ–Ω–Ω–æ. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –Ω–∞–ª–∏—á–∏—é –æ—Ñ–∏—Å–∞ –≤ –†–æ—Å—Å–∏–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥–ª—è –Ω–∞—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º.
–ü—É–Ω–∫—Ç 5. –í —Ç–µ–Ω–¥–µ—Ä–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∑–∞–ø—Ä–µ—â–µ–Ω–æ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π  —Å—É–±–ø–æ–¥—Ä—è–¥—á–∏–∫–æ–≤ (–∏ –¥—Ä. —Ç–µ—Ä–º–∏–Ω—ã —ç—Ç–æ–π –∂–µ —Å—É—Ç–∏), –∞ —Ç–∞–∫–∂–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ —Ä–∞–±–æ—Ç–µ —Ç–æ–ª—å–∫–æ —à—Ç–∞—Ç–Ω—ã–º–∏ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞–º–∏, –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —Ñ–∞–∫—Ç–∞ —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞, –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞. 
–í—ã–≤–µ–¥–∏ –≤ –Ω–∞—á–∞–ª–µ ‚Äú–û–±—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é‚Äù —Å–ª–µ–¥—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Ç–∞–±–ª–∏—Ü–µ–π ‚Äú–û–±—â–µ–µ‚Äù (–≤ 2 –∫–æ–ª–æ–Ω–∫–∏ - –≤ –ø–µ—Ä–≤–æ–π –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—É–Ω–∫—Ç–∞, –≤–æ –≤—Ç–æ—Ä–æ–π - —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ):
–ü—É–Ω–∫—Ç 1. –°—É—Ç—å —Ç–µ–Ω–¥–µ—Ä–∞ (–Ω–∞ —á—Ç–æ)
–ü—É–Ω–∫—Ç 2. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø —Ç–µ–Ω–¥–µ—Ä–∞ (–∂–∏—Ä–Ω—ã–º) –∏ –ø–æ –∫–∞–∫–∏–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Ç—ã —Ç–∞–∫ —Ä–µ—à–∏–ª (–ø—Ä–∏–≤–µ–¥–∏ —Ü–∏—Ç–∞—Ç–∞–º–∏ –≤ —ç—Ç–æ–π –∂–µ —è—á–µ–π–∫–µ)
–ü—É–Ω–∫—Ç 3. –†–∞–∑—Ä–∞–±–æ—Ç–∫—É –∫–∞–∫–∏—Ö —Å–∏—Å—Ç–µ–º, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π —Å —á–µ–º –∏ —Ç.–¥. –Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å
–ü—É–Ω–∫—Ç 4. –ö–∞–∫–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã –Ω—É–∂–Ω—ã —Å –∫–∞–∫–∏–º–∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏ –≤ –∫–∞–∫–æ–º –æ–±—ä–µ–º–µ –ª—é–¥–µ–π –∏–ª–∏ —á–µ–ª–æ–≤–µ–∫–æ-—á–∞—Å–æ–≤
–ü—É–Ω–∫—Ç 5. –ß—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å (–∫–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ —Ä–µ—à–∞—Ç—å)
–ü—É–Ω–∫—Ç 6. –ö–∞–∫–∏–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã —É —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã –∫ –Ω–∞–ª–∏—á–∏—é
–ü—É–Ω–∫—Ç 7. –°—Ä–æ–∫–∏ –æ–∫–∞–∑–∞–Ω–∏—è —É—Å–ª—É–≥
–ü—É–Ω–∫—Ç 8. –°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞ —á–∞—Å –∏–ª–∏ —Å—É–º–º–∞ —Ç–µ–Ω–¥–µ—Ä–∞
–ü—É–Ω–∫—Ç 9. –°—Ä–æ–∫–∏ –æ–ø–ª–∞—Ç—ã
–î–∞–ª–µ–µ –≤—ã–≤–µ–¥–∏ –≤ –Ω–∞—á–∞–ª–µ –∏—Ç–æ–≥–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É —Å –ø—É–Ω–∫—Ç–∞–º–∏ —á–µ–∫-–ª–∏—Å—Ç–∞ (—Ç–∞–±–ª–∏—Ü—É ‚Äù–ß–µ–∫-–ª–∏—Å—Ç –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã‚Äù). 
–ü—É–Ω–∫—Ç —á–µ–∫-–ª–∏—Å—Ç–∞
–°–æ—Å—Ç–æ—è–Ω–∏–µ
–¶–∏—Ç–∞—Ç–∞


–ï—Å–ª–∏ –≤ —Ç–µ–Ω–¥–µ—Ä–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è –Ω–∞—Å, –∑–Ω–∞—á–∏—Ç –æ–Ω –ø–æ–¥—Ö–æ–¥–∏—Ç - —Ä–∏—Å—É–π –≤ –°–æ—Å—Ç–æ—è–Ω–∏–µ –≥–∞–ª–æ—á–∫—É –∑–µ–ª–µ–Ω—ã–º —Ü–≤–µ—Ç–æ–º ‚úÖ . –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–µ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ - —Ä–∏—Å—É–π –≤ –°–æ—Å—Ç–æ—è–Ω–∏–µ –∫—Ä–µ—Å—Ç–∏–∫ –∫—Ä–∞—Å–Ω—ã–º —Ü–≤–µ—Ç–æ–º ‚ùå. –ï—Å–ª–∏ –ø—É–Ω–∫—Ç –ø–æ –Ω–∞–ª–∏—á–∏—é —Å—Ç–∞—Ç—É—Å–æ–≤, –∏ —Ç–∞–º –æ–Ω–∏ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã, –∞ –∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã, —Ç–æ —Ä–∏—Å—É–π –≤ –°–æ—Å—Ç–æ—è–Ω–∏–µ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫ –∂–µ–ª—Ç—ã–º —Ü–≤–µ—Ç–æ–º ‚ö†Ô∏è. –ï—Å–ª–∏ —Ç—ã –Ω–µ —É–≤–µ—Ä–µ–Ω - –Ω–∞—Ä–∏—Å—É–π –≤ –°–æ—Å—Ç–æ—è–Ω–∏–µ –∑–Ω–∞–∫ –≤–æ–ø—Ä–æ—Å–∞ —Å–∏–Ω–∏–π –∫—Ä—É–≥ –∏ –∑–Ω–∞–∫ –≤–æ–ø—Ä–æ—Å–∞üîµ‚ùì. –¢–∞–∫–∂–µ –¥–æ–±–∞–≤—å –∫–æ–ª–æ–Ω–∫—É ‚Äú—Ü–∏—Ç–∞—Ç–∞‚Äù- –µ—Å–ª–∏ –µ—Å—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –º—ã –Ω–µ –ø–æ—Ö–æ–¥–∏–º –∫—Ä–∏—Ç–µ—Ä–∏–π —É—á–∞—Å—Ç–∏—è, —Ç–æ –≤ —ç—Ç—É –∫–æ–ª–æ–Ω–∫—É –ø—Ä–æ–ø–∏—à–∏ —Ü–∏—Ç–∞—Ç—É –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∫–æ—Ç–æ—Ä–æ–π —Ç—ã —Ä–µ—à–∏–ª.
–ü–æ—Å–ª–µ –∏—Ç–æ–≥–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã –≤—ã–≤–µ–¥–∏ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º –≤—ã—à–µ –¥–∞–Ω–Ω—ã–º –≤ —Ä–∞–∑–¥–µ–ª  ‚Äú–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è‚Äù.
–¢–∏–ø —Ç–µ–Ω–¥–µ—Ä–∞ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º
–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ 1–°

–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –Ω–∞ 1–°, –ê–¥–∞–ø—Ç–∞—Ü–∏—è 1–°, –ê—É—Ç—Å–æ—Ä—Å–∏–Ω–≥ 1–°, –ê—É—Ç—Å—Ç–∞—Ñ—Ñ–∏–Ω–≥ 1–°, –î–æ—Ä–∞–±–æ—Ç–∫–∞ 1–°, –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è 1–°, –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø 1–°, –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è 1–°, –ù–∞—Å—Ç—Ä–æ–π–∫–∞ 1–°, –ü–µ—Ä–µ–Ω–æ—Å 1–°, –ü–µ—Ä–µ–≤–æ–¥ 1–°, –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ 1–°, –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ 1–°, –°–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ 1–°, –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π 1–°, –í–Ω–µ–¥—Ä–µ–Ω–∏–µ 1–°, –ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç 1–°, 1–°:–ü—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ 8, 1–°:ERP, 1–°:–ó–£–ü, –î–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç 1–°, –î–û 1–°
2.     –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ MS Dynamics 365 / AX (Axapta) 

MS Dynamics AX, MS Dynamics 365, Axapta, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ Axapta, –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ Dynamics, —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ Dynamics, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç Dynamics, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Dynamics
3.     –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ Python 

–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç Python, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ü–û –Ω–∞ Python, –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ Python, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –Ω–∞ Python
4.     –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ Java 

–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç Java, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ü–û –Ω–∞ Java, –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ Java, enterprise —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ Java
5.     –ê—É—Ç—Å—Ç–∞—Ñ—Ñ–∏–Ω–≥ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤ 

–∞—Ä–µ–Ω–¥–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤, —É–¥–∞–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—ã, –∏—Ç-–∞—É—Ç—Å—Ç–∞—Ñ—Ñ–∏–Ω–≥, –∞—É—Ç—Å—Ç–∞—Ñ—Ñ–∏–Ω–≥ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤, –∞—É—Ç—Å—Ç–∞—Ñ—Ñ–∏–Ω–≥ 1–°, –∞—É—Ç—Å—Ç–∞—Ñ—Ñ–∏–Ω–≥ Java, –∞—É—Ç—Å—Ç–∞—Ñ—Ñ–∏–Ω–≥ Python, –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç–±–æ—Ä, —Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞
6.     –ê—É—Ç—Å–æ—Ä—Å–∏–Ω–≥ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ 

–∞—É—Ç—Å–æ—Ä—Å–∏–Ω–≥ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –∞—É—Ç—Å–æ—Ä—Å–∏–Ω–≥ –∏—Ç, –ø–µ—Ä–µ–¥–∞—á–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ –∞—É—Ç—Å–æ—Ä—Å–∏–Ω–≥, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –Ω–∞ –∞—É—Ç—Å–æ—Ä—Å–∏–Ω–≥
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º 

–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –ø–æ,  –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è 1–° —Å –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π, —à–∏–Ω–∞ –¥–∞–Ω–Ω—ã—Ö, datareon

–û–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ 

–∫—É—Ä—Å—ã –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤ 1–°, –æ–±—É—á–µ–Ω–∏–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ, –ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤, –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é, –û–±—É—á–µ–Ω–∏–µ 1–°
Zool.ai (–í–∏–¥–µ–æ–∞–Ω–∞–ª–∏—Ç–∏–∫–∞)
–≤–∏–¥–µ–æ–∞–Ω–∞–ª–∏—Ç–∏–∫–∞, AI –≤–∏–¥–µ–æ–∞–Ω–∞–ª–∏—Ç–∏–∫–∞, –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –≤–∏–¥–µ–æ–∞–Ω–∞–ª–∏—Ç–∏–∫–∞, –∫–æ–Ω—Ç—Ä–æ–ª—å –æ—Ö—Ä–∞–Ω—ã —Ç—Ä—É–¥–∞, –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥—É–∫—Ü–∏–∏, –ø–æ–¥—Å—á–µ—Ç –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π, –∞–Ω–∞–ª–∏–∑ –ø–æ—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤, –∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—Å—Å–æ–≤–æ–π –∑–æ–Ω—ã, –º–∞—à–∏–Ω–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ, –∫–æ–Ω—Ç—Ä–æ–ª—å –∑–∞, –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è

Facereg.ru (WFM & –ë–∏–æ–º–µ—Ç—Ä–∏—è)
WFM —Å–∏—Å—Ç–µ–º–∞, —É—á–µ—Ç —Ä–∞–±–æ—á–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —É—á–µ—Ç–∞ —Ä–∞–±–æ—á–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü, Face ID, –±–∏–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø–∞, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è WFM –∏ 1–°, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∞, —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å–º–µ–Ω, –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã
Prostoskud.ru (–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –°–ö–£–î –∏ 1–°, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è 1–° –∏ –°–ö–£–î, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–µ–ª—è, –°–ö–£–î Perco, –°–ö–£–î –ë–æ–ª–∏–¥, –°–ö–£–î Parsec, 1–°: –ó–£–ü, 1–°: ERP, 1–°: –ö–ê, —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç –∫–∞–¥—Ä–æ–≤–æ–π —Å–ª—É–∂–±—ã
document_id='fe5aae47-2b4a-4257-b322-12db71fad450'
"""