На вход приходит документ, нужно разбить его на чанки и складировать в отдельную коллекцию qdrant
Потом  с помощью MCP наша модель будет обращаться к конкретной коллекции qdrant и вытаскивать нужные ей блоки текста,
 чтобы дать пользователю наиболее точный ответ

первые тесты с чанк=абзац
сделать эндпоинт на fastapi
принимает
{"area": "gpt_system_tenders", "text": "Очень длинный текст тендерной документации"}
возвращает

план:
разбить документ на маленькие смысловые части (чанки);

создать векторное представление (embedding) каждой части;

сохранить эти векторы в базе данных (Qdrant);

когда модель получает вопрос — искать в Qdrant похожие чанки;

вернуть только релевантные куски модели в prompt;

модель отвечает, используя эти найденные куски как контекст.

